---
title: "Take-home_Ex2"
editor: visual
format: 
  html:
    code-fold: true
    code-summary: "code chunk"
    fontsize: 18px
    number-sections: true
    number-depth: 2
execute:
  echo: true # all code chunk will appear
  eval: true # all code chunk will running live (be evaluated)
  warning: false # don't display warning
  message: false
---

# Background

Transport operators and urban managers face challenges in addressing urban mobility questions. Traditionally, commuter surveys have been employed, but they are expensive, time-consuming, and labor-intensive. Often, by the time these surveys are processed, the data is outdated.

With the digitization of urban infrastructure, including public buses, mass transit, and other utilities, a wealth of real-time data is now available. Technologies like GPS in vehicles and smart cards for commuters contribute to this data pool, allowing for the tracking of movement patterns in space and time.

However, the rapid accumulation of geospatial data has exceeded planners' capacity to effectively process and convert it into actionable insights, negatively affecting the return on investment in data collection and management.

# Objective

The objective is to illustrate the practical application of Geospatial Data Science and Analysis (GDSA) in decision-making processes. The task utilizes GDSA techniques to integrate and analyze data from various public sources, and then build spatial interaction models to uncover factors influencing urban mobility patterns, especially in public bus transit.

# Project Goal

### **Geospatial Data Science**

-   Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

-   With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level

    | Peak hour period             | Bus tap on time |
    |------------------------------|-----------------|
    | Weekday morning peak         | 6am to 9am      |
    | Weekday afternoon peak       | 5pm to 8pm      |
    | Weekend/holiday morning peak | 11am to 2pm     |
    | Weekend/holiday evening peak | 4pm to 7pm      |

-   Display the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).

-   Describe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).

-   Assemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.

-   Compute a distance matrix by using the analytical hexagon data derived earlier.

### **Spatial Interaction Modelling**

-   Calibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.

-   Present the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)

-   With reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual).

# Package Imports

The R packages used for the analysis are as follows:

-   sf: Analyzes and models spatial dependencies in data.

-   [**sfdep**](https://sfdep.josiahparry.com/): package which builds upon `spdep` package (compute spatial weights matrix and spatially lagged variable for instance..)

-   **tmap**: Creates thematic maps for visualizing geospatial data.

-   **tidyverse**: A collection of R packages with a unified approach for data manipulation and visualization.

-   **plotly**: Enables interactive and dynamic data visualizations.

-   **zoo**: Handles and analyzes time series data.

-   **Kendall**: Computes Kendall's tau rank correlation coefficient for assessing rank-based associations.

-   **kableExtra:** Enhances 'knitr' package's 'kable()' function for styling HTML and LaTeX tables in R Markdown. It offers advanced formatting options like row/column customization, conditional styling, and captioning, elevating tables to publication quality.

-   **ggrain:** R-package that allows you to create Raincloud plots - following the 'Grammar of Graphics' (i.e., ggplot2)

-   [**DT**](https://rstudio.github.io/DT/)**:** Create interactive html tables

-   **ggplot2:** R package for creating complex and aesthetically pleasing data visualizations using a grammar of graphics.

-   **gridExtra:** R package that provides functions for arranging multiple grid-based plots on a page, enhancing layout flexibility.

-   [**stplanr**](https://docs.ropensci.org/stplanr/)**:** is designed for addressing typical challenges in transport planning and modeling, such as determining the optimal route from point A to point B.

-   [**performance**](https://easystats.github.io/performance/)**:** for computing measures to assess model quality, which are not directly provided by R's 'base' or 'stats' packages. The primary goal of the **performance** package is to provide utilities for computing indices of model quality and goodness of fit. These include measures like r-squared (R2), root mean squared error (RMSE)

    ```{r}
    pacman::p_load(sf, sfdep, sp, tmap, tidyverse, plotly, zoo, Kendall, kableExtra, ggrain, DT, ggplot2, gridExtra,stplanr, performance, dplyr, httr, corrplot, patchwork)
    ```

# Data Description

### **Open Government Data**

For the purpose of this assignment, data from several open government sources will be used:

-   *Passenger Volume by Origin Destination Bus Stops*, *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).

-   *Master Plan 2019 Subzone Boundary*, *HDB Property Information*, *School Directory and Information* and other relevant data from [Data.gov.sg](https://beta.data.gov.sg/).

### **Specially collected data**

-   Geospatial data for Businesses, retail and services, leisure and recreation, etc provided by course instructor, Professor Kam Tin Seong.

# Data Imports And Preparation

As shown in Take-home_Ex1, we have explored that the bus data from August to October has a similar distribution throughout, hence for the purpose of this study, we will be using data from the month of September.

## Aspatial Data

Load the dataset 'Passenger Volume by Origin Destination Bus Stops' obtained from LTA Datamall into your environment using the `read_csv()` function from the `readr` package.

```{r}
bus09 <- read_csv("data/aspatial/origin_destination_bus_202309.csv")
```

head(bus09,10) %\>%

kbl() %\>%

kable_styling(

full_width = F,

bootstrap_options = c("condensed", "responsive"))

```{r}
head(bus09,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

::: {.callout-note collapse="true" title="Variables Description"}
-   **YEAR_MONTH**: Data collection month in the format of YYYY-MM.

-   **DAY_TYPE**: Weekday or Weekends/Holiday.

-   **TIME_PER_HOUR**: Hour of the day in 24 hour format.

-   **PT_TYPE**: Type of public transportation.

-   **ORIGIN_PT_CODE**: Identifier for the bus stop where the trip originated.

<!-- -->

-   **DESTINATION_PT_CODE**: Identifier for the bus stop where the trip ended.

-   **TOTAL_TRIPS**: Total number of trips recorded for each origin-destination pair.
:::

```{r}
glimpse(bus09)
```

### Data preparation

```{r}
bus09 <- bus09 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )
```

As mentioned before, ORIGIN_PT_CODE and DESTINATION_PT_CODE's data type are in character format. Because they represent bus stop locations, they should be transformed into factors (categorical data type) for further analysis.

### Data Hygiene

```{r}
count_duplicate_rows <- function(df, df_name) {
  df %>%     
    group_by_all() %>%     
    filter(n() > 1) %>%     
    ungroup() %>%     
    summarise(df_name = df_name, row_count = n()) }  
bus09_dupes  <- count_duplicate_rows(bus09, "bus09")
print(bus09_dupes)
```

```{r}
count_null_rows <- function(df, df_name) {
  df %>%
    filter(if_any(everything(), is.na)) %>%
    summarise(df_name = df_name, row_count = n())
}

bus09_nulls <- count_null_rows(bus09, "bus09")

print(bus09_nulls)
```

There are no nulls or duplicates in the data, it shows good hygiene

### Data Filtration

Following the findings from Take-home_Ex1 as well, we concur that the **Weekday Morning peak period** **(6am-9am)** is the peak period that sees the most High-High following the LISA analysis, hence will be made the target for the purpose of our task. This is done in the code block below.

```{r}
get_origin_dest <- function(data, daytype, timeinterval) {
  result <- data %>%
    filter(DAY_TYPE == daytype) %>%
    filter(TIME_PER_HOUR >= timeinterval[1] & TIME_PER_HOUR <= timeinterval[2]) %>%
    group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
    summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
    ungroup() # Ungroup for further operations if needed

  return(result)
}

# Get the data for 'WEEKDAY' between 6am and 8am
wd_morning <- get_origin_dest(bus09, 'WEEKDAY', c(6, 9))

```

```{r}
head(wd_morning,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

```{r}
wd_morning_num_rows <- nrow(wd_morning)
print(wd_morning_num_rows)
```

## Geospatial Data

Like in Take-home_Ex1, we'll be loading the Bus Stop Location shapefiles into R, resulting in an **`sf`** point object that contains 5161 points and 3 fields. Initially in the WSG84 geographical coordinate system, this data will be transformed to the EPSG 3414 projected coordinate system, which is specific to Singapore.\

```{r}
busstop <- st_read(dsn="data/geospatial", layer = "BusStop") %>% 
  st_transform(crs = 3414)
```

```{r}
busstop_num_rows <- nrow(busstop)
print(busstop_num_rows)
```

### Data Hygiene

```{r}
count_null_rows <- function(df, df_name, columns) {
  # Temporarily drop the geometry column
  df_no_geom <- df %>% 
    st_set_geometry(NULL) %>%
    select(all_of(columns))

  # Counting null rows
  null_counts <- df_no_geom %>%
    filter(if_any(everything(), is.na)) %>%
    summarise(df_name = df_name, row_count = n())

  return(null_counts)
}

# Apply function to 'busstop' dataframe for 'BUS_STOP_N' column
busstop_nulls <- count_null_rows(busstop, "busstop", c("BUS_STOP_N"))

print(busstop_nulls)


```

```{r}
busstop %>% 
  st_drop_geometry() %>% 
  group_by(BUS_STOP_N) %>%
  filter(n()>1) %>%
  ungroup() %>% 
  arrange(BUS_STOP_N)
```

There are no nulls in the BUS_STOP_N column, indicating that the primary key of the data at the very least is sound.\
\
Next we look at duplicates, it can be seen that there are indeed duplicates in the Geospatial data set. Upon investigation we can see for each corresponding **`BUS_STOP_N`,** the **`LOC_DESC`** column is mostly duplicated, which could be a result of temporary bus stops or bus stop reconstruction which results in a shift to a location in the vicinity of the original bus stop.\
This however is not the case for **`BUS_STOP_N` 47201** which sees the **`LOC_DESC`** possessing a NA value, as well as **52059**, **77329** which sees that the **`LOC_DESC`** indicating a different bus stop for their duplicated **`BUS_STOP_N`**.\
We then look to [Transitlink's](https://www.transitlink.com.sg/eservice/eguide/bscode_idx.php?bs_code=52059) website to verify the information of the identified **`BUS_STOP_N`** and will process the duplicates in the code blocks below

```{r}
# Overwrite the busstop dataframe with the filtered results
busstop <- busstop %>%
  filter(
    !(BUS_STOP_N == '47201' & is.na(LOC_DESC)) & 
    !(BUS_STOP_N == '52059' & LOC_DESC == 'OPP BLK 65') & 
    !(BUS_STOP_N == '77329' & LOC_DESC == 'Pasir Ris Central')
  )
```

```{r}
busstop <- busstop %>%
  filter(!duplicated(BUS_STOP_N))
```

```{r}
busstop %>% 
  st_drop_geometry() %>% 
  group_by(BUS_STOP_N) %>%
  filter(n()>1) %>%
  ungroup() %>% 
  arrange(BUS_STOP_N)
```

We have processed the anomalies and dropped the duplicated as per the code output above.

## Multipolygon 

```{r}
mpsz19 <- st_read(dsn='data/geospatial/',
                layer='MPSZ-2019') %>% 
  st_transform(crs=3414)
```

```{r}
head(mpsz19,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

## **Hexagon Layer**

```{r}

hx_grid = st_make_grid(busstop, cellsize = 750, what = "polygons", square = FALSE)

hx_grid_sf = st_sf(hx_grid) %>%
  rowid_to_column("hex_id")

hx_grid_sf$count_busstop = lengths(st_intersects(hx_grid_sf, busstop))

count_busstop = filter(hx_grid_sf, count_busstop > 0)

count_busstop

```

### Data Hygiene

```{r}
busstop_hx <- st_intersection(busstop, count_busstop) %>% 
  select(BUS_STOP_N, LOC_DESC,hex_id, count_busstop)

busstop_hx
```

```{r}
busstop_hx_test <- busstop_hx
busstop_hx <- busstop_hx  %>% 
  st_drop_geometry()

datatable(busstop_hx, class = 'cell-border stripe', options = list(pageLength = 5))
```

The outcomes indicated above reveal that each bus stop is positioned within at least one of the hexagonal grids.

# O-D Matrix generation and post-processes

To generate the O-D Matrix, a left join is performed twice between the **`wd_morning`** and **`busstop_hx`** dataframes.\
\
First up, the **`hex_id`**from the **`busstop_hx`** dataframe is merged into the **`wd_morning`** dataframe. This process adds the fields **`ORIGIN_GRID_ID`** and **`ORIGIN_LOC_DESC`** (to be used later in a tooltip) from **`busstop_hx`** into **`wd_morning`** .

```{r}
odmat <- left_join(wd_morning  , busstop_hx,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(O_BUSSTOP = ORIGIN_PT_CODE,
         O_HEX_ID = hex_id,
         D_BUSSTOP = DESTINATION_PT_CODE,
         O_LOC_DESC= LOC_DESC)
```

```{r}
odmat_num_rows <- nrow(odmat)
print(odmat_num_rows)
```

```{r}
duplicate <- odmat %>%
  group_by_all() %>% 
  #group_by(O_BUSSTOP, D_BUSSTOP) %>%
  filter(n()>1) %>%
  ungroup()

duplicate
```

Nothing was gained or loss during the join, the number of rows remain the same. We have also added a layer of checks to double check that there are indeed no duplicates within **`odmat`** at this point.

```{r}
odmat <- left_join(odmat, busstop_hx,
            by = c("D_BUSSTOP" = "BUS_STOP_N")) %>%
  rename(D_HEX_ID = hex_id,
         D_LOC_DESC = LOC_DESC,
         O_Count_BS = count_busstop.x,
         D_Count_BS = count_busstop.y) 
```

```{r}
odmat_num_rows <- nrow(odmat)
print(odmat_num_rows)
```

```{r}
duplicate <- odmat %>%
  group_by_all() %>% 
  #group_by(O_BUSSTOP, D_BUSSTOP) %>%
  filter(n()>1) %>%
  ungroup()

duplicate
```

Once again, nothing was gained or loss during the join, the number of rows remain the same. We have also verified once more that there are indeed no duplicates within **`odmat`** at this point.\

```{r}
# Count of NA values in each column
na_count_origin = sum(is.na(odmat$O_HEX_ID))
na_count_destin = sum(is.na(odmat$D_HEX_ID))

# Filter rows with NA values in either column
odmat_with_na <- odmat %>%
  filter(is.na(O_HEX_ID) | is.na(D_HEX_ID))

# View first few rows of filtered data
head_na_rows <- head(odmat_with_na)

# Count of NA values across all columns
na_count_all_columns <- colSums(is.na(odmat))

# Proportion of NA values in each column
na_prop_origin = mean(is.na(odmat$O_HEX_ID))
na_prop_destin = mean(is.na(odmat$D_HEX_ID))

# Tidy summary using dplyr
na_summary <- odmat %>%
  summarise(across(c(O_HEX_ID, D_HEX_ID), ~sum(is.na(.))))

# Print results
print(paste("NA count in O_HEX_ID:", na_count_origin))
print(paste("NA count in O_HEX_ID:", na_count_destin))
print("First few rows with NA values:")
print(head_na_rows)
print("NA count across all columns:")
print(na_count_all_columns)
print(paste("Proportion of NA in O_HEX_ID:", na_prop_origin))
print(paste("Proportion of NA in D_HEX_ID:", na_prop_destin))
print("Summary of NA counts in specified columns:")
print(na_summary)

```

As shown above, there are rows with null values in **`odmat`** we will now drop them

```{r}
# Removing rows with NA values
odmat <- odmat %>%
  drop_na()

```

```{r}
odmat_num_rows <- nrow(odmat)
print(odmat_num_rows)
```

Group the data by 'O_HEX_ID' and 'D_HEX_ID', and then create a new field named 'WD_MORNING'. This field should represent the total number of trips made between each unique pair of hexagons, where 'i' stands for the origin hexagon and 'j' for the destination hexagon.

```{r}
odmat <- odmat %>%
  group_by(O_HEX_ID, D_HEX_ID) %>%
  summarise(WD_MORNING = sum(TRIPS),
            O_DESC = paste(unique(O_LOC_DESC), collapse = ', '),
            D_DESC = paste(unique(D_LOC_DESC), collapse = ', ')) %>% 
  ungroup()
```

```{r}
glimpse(odmat)
```

```{r}
odmat_num_rows <- nrow(odmat)
print(odmat_num_rows)
```

Next, our focus is on calculating the distance (DIST) between each pair of hexagons for the purpose of creating visualizations that provide more clarity on the task's landscape.

## Distance Matrix

The `as.Spatial()` function is first employed to transform the **`count_busstop`** dataframe, which is currently a sf (simple features) tibble, into a **SpatialPolygonsDataFrame** (an object type from the `sp` package). This package is utilized because it is more efficient in computing the distance matrix than the `sf` package. Subsequently, the `spDists()` function from the `sp` package will be utilized to calculate the Euclidean distances between the center points of each hexagon. Lastly, we will

```{r}
count_busstop_sp <- as(count_busstop, "Spatial")
```

```{r}
dist <- sp::spDists(count_busstop_sp, 
                longlat = FALSE)
head(dist, n=c(4, 4))
```

```{r}
#rename to attach hex_id for future steps
hex_id_rename <- count_busstop$hex_id

colnames(dist) <- paste0(hex_id_rename)
rownames(dist) <- paste0(hex_id_rename)
dist[1:4,1:4]
```

## Pivot to Table

\
Next, we pviot the distance matrix (wide) to form a table (long).

For this task, the `melt()` function from the `reshape2` package will be employed. To do so, the function creates a dataframe where each row corresponds to a unique combination of row and column indices from the matrix, along with their respective values.

```{r}
dist_pairs <- reshape2::melt(dist) %>%
  rename(dist = value)
head(dist_pairs, 10)
```

```{r}
head(dist_pairs,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

```{r}
dist_pairs_num_rows <- nrow(dist_pairs)
print(dist_pairs_num_rows)

```

**Footnote:** 695,556 is derived from the original 834 rows squared, which is the max number of permutations.

```{r}
# Check if there are any rows where dist equals 0 and count them
any_zero_distances <- any(dist_pairs $dist == 0)
count_zero_distances <- sum(dist_pairs $dist == 0)

# Print the results
print(paste("Any zero distances:", any_zero_distances))
print(paste("Count of zero distances:", count_zero_distances))


```

We will filter out data where **`dist_pairs`** = 0, as those values will not reap any meaningful insights at all.

```{r}
dist_pairs %>%
  filter(dist > 0) %>%
  summary()
```

```{r}
dist_pairs$dist <- ifelse(dist_pairs$dist == 0,
                        325, dist_pairs$dist)

dist_pairs <- dist_pairs %>%
  rename(O_HEX_ID = Var1,
         D_HEX_ID = Var2)

head(dist_pairs,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

**Footnote:** We set the intra-zonal distance to be apothem of 325 apothem

```{r}
glimpse(dist_pairs)
```

## Flow Data Preparation

In this section, we aim to create a dataframe that includes the count of TRIPS and the corresponding distances for each pair of origin and destination grid IDs.

```{r}
flow_data <- odmat %>%
  group_by(O_HEX_ID, D_HEX_ID) %>% 
  summarize(TRIPS = sum(WD_MORNING)) 

head(flow_data,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

```{r}
flow_data$NoIntra <- ifelse(
  flow_data$O_HEX_ID == flow_data$D_HEX_ID, 
  0, flow_data$TRIPS)
flow_data$offset <- ifelse(
  flow_data$O_HEX_ID == flow_data$D_HEX_ID, 
  0.000001, 1)
inter_flow <- flow_data %>% 
  filter(NoIntra >0)
glimpse(inter_flow)

```

We will filter out data where **distance= 0,** as those values will not reap any meaningful insights at all.

### **Left Join passenger volume data (inter-zone) with distance value**

```{r}
inter_flow$O_HEX_ID  <- as.factor(inter_flow$O_HEX_ID)
inter_flow$D_HEX_ID  <- as.factor(inter_flow$D_HEX_ID )
dist_pairs$O_HEX_ID  <- as.factor(dist_pairs$O_HEX_ID)
dist_pairs$D_HEX_ID  <- as.factor(dist_pairs$D_HEX_ID )
FD1 <- inter_flow %>%
  left_join (dist_pairs,
             by = c("O_HEX_ID" = "O_HEX_ID",
                    "D_HEX_ID" = "D_HEX_ID"))

glimpse(FD1)
```

```{r}
head(FD1,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

## Visualising the O-D flows

Eliminating intra-zonal flows as mentioned before as it bears no meaningful insights

```{r}
odm1 <- odmat[odmat$O_HEX_ID!=odmat$D_HEX_ID	,]
```

```{r}
flowLine <- od2line(flow=FD1,
                    zones= count_busstop,
                    zone_code= 'hex_id')
```

```{r}
odm1$O_HEX_ID <- as.factor(odm1$O_HEX_ID)
odm1$D_HEX_ID <- as.factor(odm1$D_HEX_ID)

flowLine <- left_join(flowLine, odm1,
                      by = c('O_HEX_ID' = 'O_HEX_ID',
                             'D_HEX_ID' = 'D_HEX_ID'))

```

```{r}
summary(flowLine$TRIPS)
```

```{r}
quantile(flowLine$TRIPS, probs = seq(0, 1, 0.01), na.rm = TRUE)
```

We now attempt to visualize for the 3rd Quantile and up

```{r}
tmap_mode('plot')

q3q4_flowLine <- flowLine %>%
  filter(TRIPS >= 157)

count_busstop_filtered <- count_busstop %>%
  filter(hex_id %in% c(q3q4_flowLine$O_HEX_ID, q3q4_flowLine$D_HEX_ID))

tm_shape(mpsz19) +
  tm_polygons(alpha=0.3)+
              #col='black') +
#tm_shape(count_busstop_filtered) +
  #tm_polygons(alpha=0.3) +
  
  q3q4_flowLine %>%
  tm_shape() +
  tm_lines(lwd = 'TRIPS',
           style = 'quantile',
           scale= c(1, 3, 9, 15, 21, 30),
           #scale= c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha= 0.3,
           col='red') +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_layout(main.title = 'O-D Flow On Weekday Morning Peak hour' ,
            main.title.position = "center",
            main.title.size = 2.0,
            main.title.fontface = 'bold') +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar()
```

As shown the map is illegible and it is hard to distinguish or intepret from the map. Hence we now attempt to scale this down by filtering from the 99th percentile: 5446.92

```{r}
tmap_mode('plot')

q3q4_flowLine <- flowLine %>%
  filter(TRIPS >= 5446.92)

count_busstop_filtered <- count_busstop %>%
  filter(hex_id %in% c(q3q4_flowLine$O_HEX_ID, q3q4_flowLine$D_HEX_ID))

tm_shape(mpsz19) +
  tm_polygons(alpha=0.3)+
              #col='black') +
#tm_shape(count_busstop_filtered) +
  #tm_polygons(alpha=0.3) +
  
  q3q4_flowLine %>%
  tm_shape() +
  tm_lines(lwd = 'TRIPS',
           style = 'quantile',
           scale= c(1, 3, 9, 15, 21, 30),
           #scale= c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha= 0.3,
           col='red') +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_layout(main.title = 'O-D Flow On Weekday Morning Peak hour' ,
            main.title.position = "center",
            main.title.size = 2.0,
            main.title.fontface = 'bold') +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar()
```

In this map, we have excluded trips with fewer than 5446.92 trips for easier analysis as can be seen above, it is illegible when there are too many lines. Lines with greater thickness indicate a higher number of trips, while the line length signifies the distance of each trip between hexagons.

It was noted that certain bus routes transport a larger number of passengers. Prominent routes were observed in the Woodlands Checkpoint **(plausibly due to Malaysians commuting to work from beyond the causeway)** and Boon Lay regions **(understandably as it is an area which relies heavily on feeder buses to get to the main transport facilities such as the bus interchange and MRT station)**. Additionally, several routes stretching from the North to the East of Singapore are notably longer.

```{r}
tmap_mode('plot')

q3q4_flowLine <- flowLine %>%
  filter(TRIPS >= 10000)

count_busstop_filtered <- count_busstop %>%
  filter(hex_id %in% c(q3q4_flowLine$O_HEX_ID, q3q4_flowLine$D_HEX_ID))

tm_shape(mpsz19) +
  tm_polygons(alpha=0.3)+
              #col='black') +
#tm_shape(count_busstop_filtered) +
  #tm_polygons(alpha=0.3) +
  
  q3q4_flowLine %>%
  tm_shape() +
  tm_lines(lwd = 'TRIPS',
           style = 'quantile',
           scale= c(1, 3, 9, 15, 21, 30),
           #scale= c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha= 0.3,
           col='red') +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_layout(main.title = 'O-D Flow On Weekday Morning Peak hour' ,
            main.title.position = "center",
            main.title.size = 2.0,
            main.title.fontface = 'bold') +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar()
```

In this map, we have chosen to display only trips with 10,000 or more journeys, regardless of the distance traveled.

Key Observations:

1)  The most heavily trafficked flows were found in the vicinity of the Woodlands Check Point, a key point for commuter transport across the causeway.

2)  High traffic flows were frequently linked to bus or train stations as destinations, indicating that many passengers use buses to connect to further transit options. This observation suggests that the proximity of bus interchanges or train stations might play a significant role in explaining 'TRIPS' in our forthcoming Spatial Interaction Model.

# **Preparing Origin and Destination Attributes**

## Addtional Data for Propulsiveness and Attractiveness

-   Propulsive attributes are characteristics that motivate or instigate movement away from a particular place, linked to the starting point of travel. They symbolize the circumstances that exert a "push" effect on entities, prompting them to leave their current position.

-   Conversely, attractive attributes are elements that draw or lure entities to a certain place. They are connected with the journey's end point and epitomize the qualities that render a destination desirable.

### Attractiveness

#### Residential data (Attractive) 

The Weekday morning peak is a time frame where most have to get up about to do their business, whether it is going to school or work. Hence, using residential HDB data as a proxy for human density, we attempt to look at the data's Attractive capabilities

```{r}
# Load csv file
hdb <- read_csv("data/aspatial/hdb.csv")

# check the data
glimpse(hdb)
```

```{r}
glimpse(hdb)
```

```{r}
hdb_sf <- st_as_sf(hdb,
                   coords = c("lng", "lat"),
                   crs = 4326) %>%
  st_transform(crs = 3414)

# visualize the output
tm_shape(mpsz19)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(hdb_sf) +
  tm_dots(col = 'lightblue',
          id = 'building') +
  tm_layout(main.title = 'HDB Distribution Map', main.title.position = "center")

```

```{r}
summary(hdb_sf)
```

#### Business (Attractiveness) 

The Weekday morning peak is a time frame where most have to get up to head to work. Hence using business geographic density, we attempt to look at the data's Attractiveness capabilities

```{r}
business <- st_read(dsn = "data/geospatial",
                    layer = "Business") %>%
          st_transform(crs = 3414)
```

```{r}
head(business,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

```{r}
# visualize the output
# visualize the output
tm_shape(mpsz19)+
  tm_polygons(alpha = 0.01)+
  tm_shape(business)+
  tm_dots(col = 'red') +
  tm_layout(main.title = 'Business Distribution Map', main.title.position = "center")
```

#### School (Attractiveness) 

The Weekday morning peak is a time frame where most have to get up to head to school **(although we do recognize that polytechnic, ITE and University students may come at staggering timings, this is the best proxy)** . Hence using school geographic density, we attempt to look at the data's Attractiveness capabilities

```{r}
url <- "https://www.onemap.gov.sg/api/common/elastic/search"

csv <- read_csv("./data/aspatial/Generalinformationofschools.csv")
postcodes <- csv$postal_code

found <- data.frame()
not_found <- data.frame()

for(postcode in postcodes){
  query <-list('searchVal' = postcode, 'returnGeom'='Y', 'getAddrDetails'='Y', 'pageNum' = '1')
  res  <- GET(url, query=query)
  
  if((content(res)$found)!=0)
    found<-rbind(found, data.frame(content(res))[4:13])
  else {
  not_found = data.frame(postcode)
  }
}

glimpse(found)
```

```{r}
schools = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)

# manually add the Zhenghua Secondary School data
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LATITUDE"] <- 1.3887
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LONGITUDE"] <- 103.7652

# check the output
glimpse(schools)
```

```{r}
schools_sf <- schools %>%
  rename(
    latitude = "results.LATITUDE",
    longitude = "results.LONGITUDE"
  ) %>%
  select(
    postal_code, 
    school_name, 
    latitude, 
    longitude
  ) %>%
  st_as_sf(
    coords = c("longitude", "latitude"),
    crs=4326
  ) %>%
  st_transform(
    crs = 3414
  )

# visualize the output
tm_shape(mpsz19)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(schools_sf) +
  tm_dots(col = 'blue',
          id = 'building') +
  tm_layout(main.title = 'Schools Distribution Map', main.title.position = "center")
```

### Propulsiveness

#### Train Station (Propulsive)

While we recognize that train stations can be parked with Attractiveness, its presence could also be an element of Propulsiveness where commuters choose to take the train instead of the bus.

```{r}
TrainStation <- st_read(dsn="data/geospatial", layer = "RapidTransitSystemStation") %>%    st_transform(crs = 3414)
```

```{r}
# the data contain non closed ring, use st_is_valid to fix
TrainStation <- TrainStation %>%
  filter(st_is_valid(.))
         
# check the data
glimpse(TrainStation)
```

```{r}
# check for duplicates based on unique id
if_else(n_distinct(TrainStation$geometry) == nrow(TrainStation), "no duplicates detected", "possible duplicates detected")
```

```{r}
# visualize the output
tm_shape(mpsz19)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(TrainStation) +
  tm_fill(col = 'green',
          id = 'STN_NAM_DE') +
  tm_layout(main.title = 'MRT Station Distribution Map', main.title.position = "center")
```

#### Retail (Propulsive)

While we recognize that train stations can be parked with Attractiveness, its presence could also be an element of Propulsiveness where commuters are given a chance to shop in heartland malls or markets near their place of residence.

```{r}
retails <- st_read(dsn="data/geospatial", layer = "Retails") %>%    st_transform(crs = 3414)
```

```{r}
if_else(n_distinct(retails) == nrow(retails), "no duplicates detected", "possible duplicates detected")
```

```{r}
# Subset rows for duplicates and arrange by POI_NAME and POI_ST_NAM
duplicates <- retails[duplicated(retails) | duplicated(retails, fromLast = TRUE), ] %>%
  arrange(POI_NAME, POI_ST_NAM)

# Display the sorted rows with duplicatew
kable(head(duplicates, n=20))
```

```{r}
# Keep one row of the duplicates in the original dataset
retails <- retails[!duplicated(retails) | duplicated(retails, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(retails)
```

```{r}
# visualize the output
tm_shape(mpsz19) +
  tm_polygons(alpha = 0.01) +
  tm_shape(retails) +
  tm_dots(col = 'green', id = 'POI_NAME') +
  tm_layout(main.title="Retail Distribution Map", main.title.position = "center")
```

#### Distance (Propulsive)

```{r}
glimpse(flowLine)
```

```{r}
ggplot(data = flowLine,
       aes(x = WD_MORNING)) +
  geom_histogram()
```

\

```{r}
ggplot(flowLine,
       aes(x = dist, y = TRIPS)) +
  geom_point() +
  geom_hline(yintercept = 376.25, color = "red", linetype = "dashed") +
  annotate("text", x = 20000,
           y = 600, label = "95th percentile",
           hjust = -0.1, color = "red", size = 3) +
  geom_hline(yintercept = 1510, color = "purple", linetype = "dashed") +
  annotate("text", x = 20000,
           y = 1800, label = "99th percentile",
           hjust = -0.1, color = "purple", size = 3) +
  labs(title = "Number of Trips as a Function of Distance",
       x = "Distance (m)",
       y = "Number of Trips")
```

```{r}
ggplot(flowLine,
       aes(x = log(dist), y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm)
```

We observe that the further the distance travelled, the lesser the volume of trips, hence we mark distance as a propulsive factor

# Gravity Modelling

## **Propulsive Factors**

```{r}
glimpse(count_busstop)
```

```{r}

factors_holder <- count_busstop %>%
  rename(propul_count_busstop = count_busstop)

# Define a function to add push poi counts columns
add_propul_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("propul_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
propul_poi_datasets <- c("retails", "TrainStation", "flowLine")

# Call the function
factors_holder <- add_propul_poi_counts(factors_holder, propul_poi_datasets)

# Check the output
glimpse(factors_holder)

```

```{r}
hdb_sf
```

```{r}
hdb_capacity <- hdb_sf %>%
  mutate(capacity = `1room_sold` *1 +
           `2room_sold` * 2+
           `3room_sold` * 3+
           `4room_sold` * 4+
           `5room_sold` * 5+
           exec_sold * 4+
           multigen_sold * 6+
           studio_apartment_sold *1+
           `1room_rental` * 1+
           `2room_rental` * 2+
           `3room_rental` * 3+
           other_room_rental * 2) %>%
  select(blk_no, geometry, capacity)

# get hexagon_id for hdb, then sum the capacity
hdb_capacity <- st_join(hdb_capacity, count_busstop, by = NULL, join = st_within) %>%
  group_by(hex_id) %>%
  summarise(propul_est_pop = sum(capacity)) %>%
  st_drop_geometry()

# Convert hex_id to character in both dataframes
factors_holder <- factors_holder %>% mutate(hex_id = as.character(hex_id))
hdb_capacity <- hdb_capacity %>% mutate(hex_id = as.character(hex_id))

# Left join factors_holder and hdb_capacity by hex_id
factors_holder <- left_join(factors_holder, hdb_capacity, by = "hex_id") %>%
  mutate(hex_id = as.factor(hex_id)) %>%  # Convert back to factor
  mutate_at(vars(contains("propul_est_pop")), list(~coalesce(., 0.99)))  # Fill missing values with 0.99 

glimpse(factors_holder)
```

```{r}
hdb_capacity <- hdb_sf %>%
  mutate(capacity = `1room_sold` *1 +
           `2room_sold` * 2+
           `3room_sold` * 3+
           `4room_sold` * 4+
           `5room_sold` * 5+
           exec_sold * 4+
           multigen_sold * 6+
           studio_apartment_sold *1+
           `1room_rental` * 1+
           `2room_rental` * 2+
           `3room_rental` * 3+
           other_room_rental * 2) %>%
  select(blk_no, geometry, capacity)

# get hex_id for hdb, then sum the capacity
hdb_capacity <- st_join(hdb_capacity, count_busstop, by = NULL, join = st_within) %>%
  group_by(hex_id) %>%
  summarise(propul_est_pop = sum(capacity)) %>%
  st_drop_geometry()

# Convert hex_id to character in both dataframes
factors_holder <- factors_holder %>% mutate(hex_id = as.character(hex_id))
hdb_capacity <- hdb_capacity %>% mutate(hex_id = as.character(hex_id))

# Left join factors_holder and hdb_capacity by hex_id
factors_holder <- left_join(factors_holder, hdb_capacity, by = "hex_id") %>%
  mutate(hex_id = as.factor(hex_id)) %>%  # Convert back to factor
  mutate_at(vars(contains("propul_est_pop")), list(~coalesce(., 0.99)))  # Fill missing values with 0.99 

glimpse(factors_holder)
```

```{r}
hdb_capacity <- hdb_sf %>%
  mutate(capacity = `1room_sold` *1 +
           `2room_sold` * 2+
           `3room_sold` * 3+
           `4room_sold` * 4+
           `5room_sold` * 5+
           exec_sold * 4+
           multigen_sold * 6+
           studio_apartment_sold *1+
           `1room_rental` * 1+
           `2room_rental` * 2+
           `3room_rental` * 3+
           other_room_rental * 2) %>%
  select(blk_no, geometry, capacity)

# get hex_id for hdb, then sum the capacity
hdb_capacity <- st_join(hdb_capacity, count_busstop, by = NULL, join = st_within) %>%
  group_by(hex_id) %>%
  summarise(propul_est_pop = sum(capacity)) %>%
  st_drop_geometry()

# Convert hex_id to character in both dataframes
factors_holder <- factors_holder %>% mutate(hex_id = as.character(hex_id))
hdb_capacity <- hdb_capacity %>% mutate(hex_id = as.character(hex_id))

# Left join factors_holder and hdb_capacity by hex_id
factors_holder <- left_join(factors_holder, hdb_capacity, by = "hex_id") %>%
  mutate(hex_id = as.factor(hex_id)) %>%  # Convert back to factor
  mutate_at(vars(contains("propul_est_pop")), list(~coalesce(., 0.99)))  # Fill missing values with 0.99 

glimpse(factors_holder)
```

## **Attractive Factors**

```{r}
# define function to add att poi counts columns
add_att_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("att_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
att_poi_datasets <- c("hdb_sf", "business", "schools_sf")

# Call the function
factors_holder <- add_att_poi_counts(factors_holder, att_poi_datasets)

glimpse(factors_holder)
```

## Final Dataframe

```{r}
n_distinct(factors_holder$hex_id)
```

```{r}
glimpse(factors_holder)
```

```{r}
# Convert hex_id in factors_holder to integer
factors_holder <- factors_holder %>%
  mutate(hex_id = as.factor(hex_id))


# Perform the left joins with matching data types
final_df <- left_join(FD1, factors_holder %>% select(starts_with("att_"), hex_id), by = c("O_HEX_ID" = "hex_id"))
final_df <- left_join(final_df, factors_holder %>% select(starts_with("propul_"), hex_id), by = c("D_HEX_ID" = "hex_id"))

# Check the output
glimpse(final_df)

```

```{r}
write_rds(final_df, "data/rds/final_df_the2.rds")
```

```{r}
final_df <- na.omit(final_df)
```

```{r}
head(final_df,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

## Correlation Matrix

```{r}
# Select columns by indices
numeric_cols_indices <- c(3, 6, 7, 8, 9, 11, 12, 13, 14, 17)
correlation_df <- final_df[, numeric_cols_indices]

# Check the structure of the new dataframe
str(correlation_df)

# Calculate the correlation matrix, handling NA values
correlation_matrix <- cor(correlation_df, use = "pairwise.complete.obs")

# Load the corrplot package if not already loaded
if (!requireNamespace("corrplot", quietly = TRUE)) {
  install.packages("corrplot")
}
library(corrplot)

# Generate the correlation plot
corrplot.mixed(correlation_matrix,
               lower = "color",
               upper = "number",
               tl.pos = "lt",
               tl.col = "black",
               tl.cex = 0.7,
               number.cex = 0.4)
```

There no indications of overly high correlations which could result in multicollinearity, albeit the correlations not showing very strong indications of being correlated.

## Unconstrained

```{r}
final_df <- final_df %>%
  mutate(
    propul_count_busstop = ifelse(propul_count_busstop <= 0, 0.01, propul_count_busstop),
    dist = ifelse(dist <= 0, 0.01, dist),
    att_hdb_sf_count = ifelse(att_hdb_sf_count <= 0, 0.01, att_hdb_sf_count),
    att_business_count = ifelse(att_business_count <= 0, 0.01, att_business_count),
    att_schools_sf_count = ifelse(att_schools_sf_count <= 0, 0.01, att_schools_sf_count),
    propul_retails_count = ifelse(propul_retails_count <= 0, 0.01, propul_retails_count),
    propul_TrainStation_count = ifelse(propul_TrainStation_count <= 0, 0.01, propul_TrainStation_count),
    propul_flowLine_count = ifelse(propul_flowLine_count <= 0, 0.01, propul_flowLine_count),
    propul_est_pop = ifelse(propul_est_pop <= 0, 0.01, propul_est_pop)
  )

# Fit the model
uncSIM <- glm(formula = TRIPS ~ 
                log(dist) +
                log(att_hdb_sf_count) +
                log(att_business_count) +
                log(att_schools_sf_count) +
                log(propul_retails_count) +
                log(propul_TrainStation_count) +
                log(propul_flowLine_count) +
                log(propul_est_pop),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(uncSIM)
```

## Origin Constrained

```{r}
final_df <- final_df %>%
  mutate(
    propul_count_busstop = ifelse(propul_count_busstop <= 0, 0.01, propul_count_busstop),
    dist = ifelse(dist <= 0, 0.01, dist),
    att_hdb_sf_count = ifelse(att_hdb_sf_count <= 0, 0.01, att_hdb_sf_count),
    att_business_count = ifelse(att_business_count <= 0, 0.01, att_business_count),
    att_schools_sf_count = ifelse(att_schools_sf_count <= 0, 0.01, att_schools_sf_count),
    propul_retails_count = ifelse(propul_retails_count <= 0, 0.01, propul_retails_count),
    propul_TrainStation_count = ifelse(propul_TrainStation_count <= 0, 0.01, propul_TrainStation_count),
    propul_flowLine_count = ifelse(propul_flowLine_count <= 0, 0.01, propul_flowLine_count),
    propul_est_pop = ifelse(propul_est_pop <= 0, 0.01, propul_est_pop)
  )

# Fit the model (corrected)
orcSIM <- glm(formula = TRIPS ~ 
                log(dist) +
                log(att_hdb_sf_count) +
                log(att_business_count) +
                log(att_schools_sf_count),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(orcSIM)
```

## Destination Constrained

```{r}
final_df <- final_df %>%
  mutate(
    propul_count_busstop = ifelse(propul_count_busstop <= 0, 0.01, propul_count_busstop),
    dist = ifelse(dist <= 0, 0.01, dist),
    att_hdb_sf_count = ifelse(att_hdb_sf_count <= 0, 0.01, att_hdb_sf_count),
    att_business_count = ifelse(att_business_count <= 0, 0.01, att_business_count),
    att_schools_sf_count = ifelse(att_schools_sf_count <= 0, 0.01, att_schools_sf_count),
    propul_retails_count = ifelse(propul_retails_count <= 0, 0.01, propul_retails_count),
    propul_TrainStation_count = ifelse(propul_TrainStation_count <= 0, 0.01, propul_TrainStation_count),
    propul_flowLine_count = ifelse(propul_flowLine_count <= 0, 0.01, propul_flowLine_count),
    propul_est_pop = ifelse(propul_est_pop <= 0, 0.01, propul_est_pop)
  )

# Fit the model
decSIM <- glm(formula = TRIPS ~ 
                log(dist) +
                log(propul_retails_count) +
                log(propul_TrainStation_count) +
                log(propul_flowLine_count) +
                log(propul_est_pop),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(decSIM)
```

## Doubly Constrained

```{r}
docSIM <- glm(formula = TRIPS ~ 
                O_HEX_ID +
                D_HEX_ID +
                log(dist),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(docSIM)
```

# Comparing the results and conclusions

## **Performance Table**

```{r}
model_list <- list(
  Unconstrained = uncSIM,
  Origin_Constrained = orcSIM,
  Destination_Constrained = decSIM,
  Doubly_Constrained = docSIM
)

# Compare performance with multiple metrics
compare_performance(model_list, metrics = c("AIC", "BIC", "RMSE"))
```

**AIC (Akaike Information Criterion)**: - AIC is a measure of the relative quality of statistical models for a given set of data. Lower AIC values indicate a model is considered better because it achieves a good fit with fewer parameters. - The weights in parentheses suggest the probability that each model is the best among the set of models evaluated. A weight near 1 indicates a model is most likely to be the best, while a weight near 0 indicates it is less likely. - According to the AIC weights, the Doubly_Constrained model is overwhelmingly the preferred model (\>0.999 probability of being the best model). The other three models have weights less than 0.001, indicating they are less likely to be the best.

**BIC (Bayesian Information Criterion)**: - BIC is similar to AIC but includes a penalty term for the number of parameters in the model, which tends to favor simpler models. As with AIC, lower BIC values are better. - The weights here, like the AIC weights, indicate that the Doubly_Constrained model is far more likely to be the best model according to the BIC as well.

**RMSE (Root Mean Square Error)**: - RMSE measures the model's prediction error, which is the square root of the average squared differences between the predicted and observed values. A lower RMSE value indicates a better fit. - The RMSE values reinforce the AIC and BIC findings, with the Doubly_Constrained model having a significantly lower RMSE (1043.199) than the other models. This means its predictions are, on average, closer to the observed data.

**Interpretation**: - The Doubly_Constrained model outperforms the other models in terms of fit and complexity as measured by AIC and BIC. It is also the best model in terms of predictive accuracy, with the lowest RMSE. - The Origin_Constrained model is slightly better than the Unconstrained and Destination_Constrained models, but it still falls short when compared to the Doubly_Constrained model. - The similar performance of the Unconstrained and Destination_Constrained models, both with AIC and BIC weights of less than 0.001, suggests that they might be overfitting the data or including too many parameters that do not contribute to a better fit. - Overall, the analysis suggests that accounting for constraints on both the origin and destination of trips provides the most accurate and efficient model for predicting trip distribution in this context.

## Visualization of fitted values

```{r}

# Function to round fitted values and create a data frame
round_and_rename <- function(sim_data, sim_name) {
  as.data.frame(sim_data$fitted.values) %>%
    round(digits = 0) %>%
    setNames(paste0(sim_name, "_TRIPS"))
}

# Round and rename fitted values for each simulation
# Make sure the variables orcSIM, decSIM, etc. are correctly defined in your context
uncSIM_fitted <- round_and_rename(orcSIM, "uncSIM")
orcSIM_fitted <- round_and_rename(orcSIM, "orcSIM")
decSIM_fitted <- round_and_rename(decSIM, "decSIM")
docSIM_fitted <- round_and_rename(docSIM, "docSIM")

# Combine the rounded and renamed fitted values
# Ensure final_df is defined and contains relevant data
final_df_viz <- final_df %>%
  cbind(uncSIM_fitted, orcSIM_fitted, decSIM_fitted, docSIM_fitted)

# Create a function to generate a ggplot
generate_ggplot <- function(data, x_col, title) {
  ggplot(data = data, aes_string(x = x_col, y = "TRIPS")) +
    geom_point(
      aes(size = TRIPS / 10000),
      alpha = .6,
      shape = 21
    ) +
    xlim(0, 50000) +
    geom_smooth(
      method = "lm",
      se = TRUE,
      color = "blue"
    ) +
    labs(title = title) +
    theme(
      plot.title = element_text(size = 10),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.y = element_blank(),
      legend.position = "none"  # Hide the legend
    )
}

# Generate ggplots for each simulation
p_unc <- generate_ggplot(final_df_viz, "uncSIM_TRIPS", "Unconstrained")
p_orc <- generate_ggplot(final_df_viz, "orcSIM_TRIPS", "Origin-constrained")
p_dec <- generate_ggplot(final_df_viz, "decSIM_TRIPS", "Destination-constrained")
p_doc <- generate_ggplot(final_df_viz, "docSIM_TRIPS", "Doubly-constrained")

# Combine the plots using patchwork
combined_plot <- p_unc + p_orc + p_dec + p_doc 

# Print the combined plot
print(combined_plot)

```

1.  **Unconstrained Model**:
    -   This model shows a lot of variation with many points densely packed at the lower end of the `uncSIM_TRIPS` axis, suggesting a large number of trips with smaller values.
    -   The spread of the points suggests that there's no restriction on the flow of trips between origins and destinations, leading to a wide range of trip volumes.
2.  **Origin-constrained Model**:
    -   The points are densely concentrated at the lower end of the `orcSIM_TRIPS` axis, similar to the unconstrained model, which indicates a significant number of trips with lower volumes.
    -   The points do not show a clear pattern, which suggests that even with the constraint on the origin, the trip volumes vary widely.
3.  **Destination-constrained Model**:
    -   Like the other two models above, there is a dense cluster of points at the lower end of the `decSIM_TRIPS` axis.
    -   There's a noticeable line of points extending up, which may suggest that for certain destinations, the number of trips increases with the trip volume. However, the overall pattern is still quite varied.
4.  **Doubly-constrained Model**:
    -   This model exhibits a clearer pattern with points aligning along a trend line, represented by the blue line.
    -   The size of the points, representing trip volume, is more varied and extends towards higher trip volumes along the trend line, indicating a more predictable relationship between the simulation's inputs and the number of trips.
    -   This pattern suggests that when both origin and destination constraints are applied, the trip volumes become more predictable, and there is a trend that can be modeled, which is reflected by the regression line.

**Additional Insights**:

-   The presence of many small circles, particularly in the unconstrained and origin-constrained models, indicates a large number of trips with smaller volumes.
-   The doubly-constrained model seems to fit the data best, as indicated by the alignment of the points with the trend line, suggesting that considering both origins and destinations constraints provides a more accurate representation of the trip distribution.
-   The unconstrained and origin-constrained models show that without destination constraints, there's a wide distribution of trips, which could imply that destinations vary widely and some are much more popular than others.
