---
title: "Take-home_Ex2"
editor: visual
format: 
  html:
    code-fold: true
    code-summary: "code chunk"
    fontsize: 18px
    number-sections: true
    number-depth: 2
execute:
  echo: true # all code chunk will appear
  eval: true # all code chunk will running live (be evaluated)
  warning: false # don't display warning
  message: false
---

# Background

Transport operators and urban managers face challenges in addressing urban mobility questions. Traditionally, commuter surveys have been employed, but they are expensive, time-consuming, and labor-intensive. Often, by the time these surveys are processed, the data is outdated.

With the digitization of urban infrastructure, including public buses, mass transit, and other utilities, a wealth of real-time data is now available. Technologies like GPS in vehicles and smart cards for commuters contribute to this data pool, allowing for the tracking of movement patterns in space and time.

However, the rapid accumulation of geospatial data has exceeded planners' capacity to effectively process and convert it into actionable insights, negatively affecting the return on investment in data collection and management.

# Objective

The objective is to illustrate the practical application of Geospatial Data Science and Analysis (GDSA) in decision-making processes. The task utilizes GDSA techniques to integrate and analyze data from various public sources, and then build spatial interaction models to uncover factors influencing urban mobility patterns, especially in public bus transit.

# Project Goal

### **Geospatial Data Science**

-   Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

-   With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level

    | Peak hour period             | Bus tap on time |
    |------------------------------|-----------------|
    | Weekday morning peak         | 6am to 9am      |
    | Weekday afternoon peak       | 5pm to 8pm      |
    | Weekend/holiday morning peak | 11am to 2pm     |
    | Weekend/holiday evening peak | 4pm to 7pm      |

-   Display the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).

-   Describe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).

-   Assemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.

-   Compute a distance matrix by using the analytical hexagon data derived earlier.

### **Spatial Interaction Modelling**

-   Calibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.

-   Present the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)

-   With reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual).

# Package Imports

The R packages used for the analysis are as follows:

-   sf: Analyzes and models spatial dependencies in data.

-   [**sfdep**](https://sfdep.josiahparry.com/): package which builds upon `spdep` package (compute spatial weights matrix and spatially lagged variable for instance..)

-   **tmap**: Creates thematic maps for visualizing geospatial data.

-   **tidyverse**: A collection of R packages with a unified approach for data manipulation and visualization.

-   **plotly**: Enables interactive and dynamic data visualizations.

-   **zoo**: Handles and analyzes time series data.

-   **Kendall**: Computes Kendall's tau rank correlation coefficient for assessing rank-based associations.

-   **kableExtra:** Enhances 'knitr' package's 'kable()' function for styling HTML and LaTeX tables in R Markdown. It offers advanced formatting options like row/column customization, conditional styling, and captioning, elevating tables to publication quality.

-   **ggrain:** R-package that allows you to create Raincloud plots - following the 'Grammar of Graphics' (i.e., ggplot2)

-   [**DT**](https://rstudio.github.io/DT/)**:** Create interactive html tables

-   **ggplot2:** R package for creating complex and aesthetically pleasing data visualizations using a grammar of graphics.

-   **gridExtra:** R package that provides functions for arranging multiple grid-based plots on a page, enhancing layout flexibility.

-   [**stplanr**](https://docs.ropensci.org/stplanr/)**:** is designed for addressing typical challenges in transport planning and modeling, such as determining the optimal route from point A to point B.

-   [**performance**](https://easystats.github.io/performance/)**:** for computing measures to assess model quality, which are not directly provided by R's 'base' or 'stats' packages. The primary goal of the **performance** package is to provide utilities for computing indices of model quality and goodness of fit. These include measures like r-squared (R2), root mean squared error (RMSE)

    ```{r}
    pacman::p_load(sf, sfdep, tmap, tidyverse, plotly, zoo, Kendall, kableExtra, ggrain, DT, ggplot2, gridExtra,stplanr, performance, dplyr)
    ```

# Data Description

### **Open Government Data**

For the purpose of this assignment, data from several open government sources will be used:

-   *Passenger Volume by Origin Destination Bus Stops*, *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).

-   *Master Plan 2019 Subzone Boundary*, *HDB Property Information*, *School Directory and Information* and other relevant data from [Data.gov.sg](https://beta.data.gov.sg/).

### **Specially collected data**

-   Geospatial data for Businesses, retail and services, leisure and recreation, etc provided by course instructor, Professor Kam Tin Seong.

# Data Imports And Preparation

As shown in Take-home_Ex1, we have explored that the bus data from August to October has a similar distribution throughout, hence for the purpose of this study, we will be using data from the month of September.

## Aspatial Data

Load the dataset 'Passenger Volume by Origin Destination Bus Stops' obtained from LTA Datamall into your environment using the `read_csv()` function from the `readr` package.

```{r}
bus09 <- read_csv("data/aspatial/origin_destination_bus_202309.csv")
```

head(bus09,10) %\>%

kbl() %\>%

kable_styling(

full_width = F,

bootstrap_options = c("condensed", "responsive"))

```{r}
head(bus09,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

::: {.callout-note collapse="true" title="Variables Description"}
-   **YEAR_MONTH**: Data collection month in the format of YYYY-MM.

-   **DAY_TYPE**: Weekday or Weekends/Holiday.

-   **TIME_PER_HOUR**: Hour of the day in 24 hour format.

-   **PT_TYPE**: Type of public transportation.

-   **ORIGIN_PT_CODE**: Identifier for the bus stop where the trip originated.

```{=html}
<!-- -->
```
-   **DESTINATION_PT_CODE**: Identifier for the bus stop where the trip ended.

-   **TOTAL_TRIPS**: Total number of trips recorded for each origin-destination pair.
:::

```{r}
glimpse(bus09)
```

### Data preparation 

```{r}
bus09 <- bus09 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )
```

As mentioned before, ORIGIN_PT_CODE and DESTINATION_PT_CODE's data type are in character format. Because they represent bus stop locations, they should be transformed into factors (categorical data type) for further analysis.

### Data Hygiene

```{r}
count_duplicate_rows <- function(df, df_name) {
  df %>%     
    group_by_all() %>%     
    filter(n() > 1) %>%     
    ungroup() %>%     
    summarise(df_name = df_name, row_count = n()) }  
bus09_dupes  <- count_duplicate_rows(bus09, "bus09")
print(bus09_dupes)
```

```{r}
count_null_rows <- function(df, df_name) {
  df %>%
    filter(if_any(everything(), is.na)) %>%
    summarise(df_name = df_name, row_count = n())
}

bus09_nulls <- count_null_rows(bus09, "bus09")

print(bus09_nulls)
```

There are no nulls or duplicates in the data, it shows good hygiene

### Data Filtration 

Following the findings from Take-home_Ex1 as well, we concur that the **Weekday Morning peak period** **(6am-9am)** is the peak period that sees the most High-High following the LISA analysis, hence will be made the target for the purpose of our task. This is done in the code block below.

```{r}
get_origin_dest <- function(data, daytype, timeinterval) {
  result <- data %>%
    filter(DAY_TYPE == daytype) %>%
    filter(TIME_PER_HOUR >= timeinterval[1] & TIME_PER_HOUR <= timeinterval[2]) %>%
    group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
    summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
    ungroup() # Ungroup for further operations if needed

  return(result)
}

# Get the data for 'WEEKDAY' between 6am and 8am
wd_morning <- get_origin_dest(bus09, 'WEEKDAY', c(6, 9))

```

```{r}
head(wd_morning,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

```{r}
wd_morning
```

## Geospatial Data

Like in Take-home_Ex1, we'll be loading the Bus Stop Location shapefiles into R, resulting in an **`sf`** point object that contains 5161 points and 3 fields. Initially in the WSG84 geographical coordinate system, this data will be transformed to the EPSG 3414 projected coordinate system, which is specific to Singapore.\

```{r}
busstop <- st_read(dsn="data/geospatial", layer = "BusStop") %>% 
  st_transform(crs = 3414)
```

```{r}
busstop
```

### Data Hygiene

```{r}
count_null_rows <- function(df, df_name, columns) {
  # Temporarily drop the geometry column
  df_no_geom <- df %>% 
    st_set_geometry(NULL) %>%
    select(all_of(columns))

  # Counting null rows
  null_counts <- df_no_geom %>%
    filter(if_any(everything(), is.na)) %>%
    summarise(df_name = df_name, row_count = n())

  return(null_counts)
}

# Apply function to 'busstop' dataframe for 'BUS_STOP_N' column
busstop_nulls <- count_null_rows(busstop, "busstop", c("BUS_STOP_N"))

print(busstop_nulls)


```

```{r}
busstop %>% 
  st_drop_geometry() %>% 
  group_by(BUS_STOP_N) %>%
  filter(n()>1) %>%
  ungroup() %>% 
  arrange(BUS_STOP_N)
```

There are no nulls in the BUS_STOP_N column, indicating that the primary key of the data at the very least is sound. \
\
Next we look at duplicates, it can be seen that there are indeed duplicates in the Geospatial data set. Upon investigation we can see for each corresponding **`BUS_STOP_N`,** the **`LOC_DESC`** column is mostly duplicated, which could be a result of temporary bus stops or bus stop reconstruction which results in a shift to a location in the vicinity of the original bus stop. \
This however is not the case for **`BUS_STOP_N` 47201** which sees the **`LOC_DESC`** possessing a NA value, as well as **52059**, **77329** which sees that the **`LOC_DESC`** indicating a different bus stop for their duplicated **`BUS_STOP_N`**. \
We then look to [Transitlink's](https://www.transitlink.com.sg/eservice/eguide/bscode_idx.php?bs_code=52059) website to verify the information of the identified **`BUS_STOP_N`** and will process the duplicates in the code blocks below

```{r}
# Overwrite the busstop dataframe with the filtered results
busstop <- busstop %>%
  filter(
    !(BUS_STOP_N == '47201' & is.na(LOC_DESC)) & 
    !(BUS_STOP_N == '52059' & LOC_DESC == 'OPP BLK 65') & 
    !(BUS_STOP_N == '77329' & LOC_DESC == 'Pasir Ris Central')
  )
```

```{r}
busstop <- busstop %>%
  filter(!duplicated(BUS_STOP_N))
```

```{r}
busstop %>% 
  st_drop_geometry() %>% 
  group_by(BUS_STOP_N) %>%
  filter(n()>1) %>%
  ungroup() %>% 
  arrange(BUS_STOP_N)
```

We have processed the anomalies and dropped the duplicated as per the code output above.

## MultipolygonÂ 

```{r}
mpsz19 <- st_read(dsn='data/geospatial/',
                layer='MPSZ-2019') %>% 
  st_transform(crs=3414)
```

```{r}
head(mpsz19,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

##  **Hexagon Layer **

```{r}

hx_grid = st_make_grid(busstop, cellsize = 750, what = "polygons", square = FALSE)

hx_grid_sf = st_sf(hx_grid) %>%
  rowid_to_column("hex_id")

hx_grid_sf$count_busstop = lengths(st_intersects(hx_grid_sf, busstop))

count_busstop = filter(hx_grid_sf, count_busstop > 0)

count_busstop

```

### Data Hygiene

```{r}
busstop_hx <- st_intersection(busstop, count_busstop) %>% 
  select(BUS_STOP_N, LOC_DESC,hex_id, count_busstop)

busstop_hx
```

```{r}
busstop_hx_test <- busstop_hx
busstop_hx <- busstop_hx  %>% 
  st_drop_geometry()

datatable(busstop_hx, class = 'cell-border stripe', options = list(pageLength = 5))
```

The outcomes indicated above reveal that each bus stop is positioned within at least one of the hexagonal grids.

# O-D Matrix generation

To generate the O-D Matrix, a left join is performed twice between the **`wd_morning`** and **`busstop_hx`** dataframes. \
\
First up, the **`hex_id`**from the **`busstop_hx`** dataframe is merged into the **`wd_morning`** dataframe. This process adds the fields **`ORIGIN_GRID_ID`** and **`ORIGIN_LOC_DESC`** (to be used later in a tooltip) from **`busstop_hx`** into **`wd_morning`** .

```{r}
odmat <- left_join(wd_morning  , busstop_hx,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(O_BUSSTOP = ORIGIN_PT_CODE,
         O_HEX_ID = hex_id,
         D_BUSSTOP = DESTINATION_PT_CODE,
         O_LOC_DESC= LOC_DESC)
```

```{r}
odmat
```

```{r}
duplicate <- odmat %>%
  group_by_all() %>% 
  #group_by(O_BUSSTOP, D_BUSSTOP) %>%
  filter(n()>1) %>%
  ungroup()

duplicate
```

Nothing was gained or loss during the join, the number of rows remain the same. We have also added a layer of checks to double check that there are indeed no duplicates within **`odmat`** at this point.

```{r}
odmat <- left_join(odmat, busstop_hx,
            by = c("D_BUSSTOP" = "BUS_STOP_N")) %>%
  rename(D_HEX_ID = hex_id,
         D_LOC_DESC = LOC_DESC,
         O_Count_BS = count_busstop.x,
         D_Count_BS = count_busstop.y) 
```

```{r}
odmat
```

```{r}
duplicate <- odmat %>%
  group_by_all() %>% 
  #group_by(O_BUSSTOP, D_BUSSTOP) %>%
  filter(n()>1) %>%
  ungroup()

duplicate
```

Once again, nothing was gained or loss during the join, the number of rows remain the same. We have also verified once more that there are indeed no duplicates within **`odmat`** at this point.\

```{r}
# Count of NA values in each column
na_count_origin = sum(is.na(odmat$O_HEX_ID))
na_count_destin = sum(is.na(odmat$D_HEX_ID))

# Filter rows with NA values in either column
odmat_with_na <- odmat %>%
  filter(is.na(O_HEX_ID) | is.na(D_HEX_ID))

# View first few rows of filtered data
head_na_rows <- head(odmat_with_na)

# Count of NA values across all columns
na_count_all_columns <- colSums(is.na(odmat))

# Proportion of NA values in each column
na_prop_origin = mean(is.na(odmat$O_HEX_ID))
na_prop_destin = mean(is.na(odmat$D_HEX_ID))

# Tidy summary using dplyr
na_summary <- odmat %>%
  summarise(across(c(O_HEX_ID, D_HEX_ID), ~sum(is.na(.))))

# Print results
print(paste("NA count in O_HEX_ID:", na_count_origin))
print(paste("NA count in O_HEX_ID:", na_count_destin))
print("First few rows with NA values:")
print(head_na_rows)
print("NA count across all columns:")
print(na_count_all_columns)
print(paste("Proportion of NA in O_HEX_ID:", na_prop_origin))
print(paste("Proportion of NA in D_HEX_ID:", na_prop_destin))
print("Summary of NA counts in specified columns:")
print(na_summary)

```

As shown above, there are rows with null values in **`odmat`** we will now drop them

```{r}
# Removing rows with NA values
odmat <- odmat %>%
  drop_na()

```

```{r}
odmat
```

Group the data by 'O_HEX_ID' and 'D_HEX_ID', and then create a new field named 'WD_MORNING'. This field should represent the total number of trips made between each unique pair of hexagons, where 'i' stands for the origin hexagon and 'j' for the destination hexagon.

```{r}
odmat <- odmat %>%
  group_by(O_HEX_ID, D_HEX_ID) %>%
  summarise(WD_MORNING = sum(TRIPS),
            O_DESC = paste(unique(O_LOC_DESC), collapse = ', '),
            D_DESC = paste(unique(D_LOC_DESC), collapse = ', ')) %>% 
  ungroup()
```

```{r}
odmat
```

Next, our focus is on calculating the distance (DIST) between each pair of hexagons for the purpose of creating visualizations that provide more clarity on the task's landscape.

# Distance Matrix  

The `as.Spatial()` function is first employed to transform the **`count_busstop`** dataframe, which is currently a sf (simple features) tibble, into a **SpatialPolygonsDataFrame** (an object type from the `sp` package). This package is utilized because it is more efficient in computing the distance matrix than the `sf` package. Subsequently, the `spDists()` function from the `sp` package will be utilized to calculate the Euclidean distances between the center points of each hexagon. Lastly, we will

```{r}
count_busstop_sp <- as(count_busstop, "Spatial")
```

```{r}
dist <- sp::spDists(count_busstop_sp, 
                longlat = FALSE)
head(dist, n=c(4, 4))
```

```{r}
#rename to attach hex_id for future steps
hex_id_rename <- count_busstop$hex_id

colnames(dist) <- paste0(hex_id_rename)
rownames(dist) <- paste0(hex_id_rename)
dist[1:4,1:4]
```

## Pivot to Table

\
Next, we pviot the distance matrix (wide) to form a table (long).

For this task, the `melt()` function from the `reshape2` package will be employed. To do so, the function creates a dataframe where each row corresponds to a unique combination of row and column indices from the matrix, along with their respective values.

```{r}
dist_pairs <- reshape2::melt(dist) %>%
  rename(dist = value)
head(dist_pairs, 10)
```

```{r}
head(dist_pairs,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

```{r}
dist_pairs
```

**Footnote:** 695,556 is derived from the original 834 rows squared, which is the max number of permutations.

```{r}
# Check if there are any rows where dist equals 0 and count them
any_zero_distances <- any(dist_pairs $dist == 0)
count_zero_distances <- sum(dist_pairs $dist == 0)

# Print the results
print(paste("Any zero distances:", any_zero_distances))
print(paste("Count of zero distances:", count_zero_distances))


```

We will filter out data where **`dist_pairs`** = 0, as those values will not reap any meaningful insights at all.

```{r}
dist_pairs %>%
  filter(dist > 0) %>%
  summary()
```

```{r}
dist_pairs$dist <- ifelse(dist_pairs$dist == 0,
                        325, dist_pairs$dist)

dist_pairs <- dist_pairs %>%
  rename(O_HEX_ID = Var1,
         D_HEX_ID = Var2)

head(dist_pairs,10) %>%
  kbl() %>%
  kable_styling(
    full_width = F, 
    bootstrap_options = c("condensed", "responsive"))
```

We set the intra-zonal distance to be apothem of 325 apothem
